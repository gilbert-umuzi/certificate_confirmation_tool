{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1cc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3556154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# For Python packages\n",
    "!pip install --upgrade PyPDF2\n",
    "!pip install --upgrade pdf2image\n",
    "!pip install --upgrade pdfplumber\n",
    "!pip install --upgrade pytesseract\n",
    "!pip install --upgrade fuzzywuzzy\n",
    "\n",
    "!pip install python-Levenshtein\n",
    "!pip install pandas\n",
    "!pip install scikit-image\n",
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "\n",
    "# Run in terminal\n",
    "# brew install poppler\n",
    "# brew install tesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import re  # For regular expressions\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfReader\n",
    "from PyPDF2 import PdfFileReader\n",
    "from pdf2image import convert_from_path\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "import requests  # For downloading files\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from fuzzywuzzy import fuzz  # for text similarity\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File types that are supported\n",
    "SUPPORTED_EXTENSIONS = ['.pdf', '.png', '.jpg', '.jpeg', '.bmp', '.tiff']\n",
    "\n",
    "# Names of the courses / certificates that are being evaluated\n",
    "certificate_names = [\n",
    "    'Boost Your Career with SAP Skills My Learning screenshot',\n",
    "    'Learn how to learn (Google)',\n",
    "    'Business Communication (Google)',\n",
    "    'Communicate your ideas through storytelling and design (Google)',\n",
    "    'Get started with Microsoft Teams',\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172466f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When working in Colab, mount your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where attachments are stored\n",
    "attachments_dir = '/Users/gilbert/Downloads/attachments '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize filepath and check it (requires Helper Functions to be loaded)\n",
    "attachments_dir = sanitize_path(attachments_dir)\n",
    "\n",
    "# Check if attachments directory exists\n",
    "if check_file_exists(attachments_dir):\n",
    "    print(f\"The directory {attachments_dir} exists.\")\n",
    "    \n",
    "    # Show the list of subfolders\n",
    "    with os.scandir(attachments_dir) as entries:\n",
    "        subfolders = [entry.name for entry in entries if entry.is_dir()]\n",
    "    print(f\"Subfolders: {subfolders}\")\n",
    "else:\n",
    "    print(f\"The directory {attachments_dir} does not exist. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ded21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load template certificates into their respective folders\n",
    "\n",
    "# Define the folder paths for each certificate type\n",
    "certificate_folders = {\n",
    "    'Boost Your Career with SAP Skills My Learning screenshot': '/Users/gilbert/Downloads/example_certs/Boost Your Career with SAP Skills My Learning screenshot',\n",
    "    'Learn how to learn (Google)': '/Users/gilbert/Downloads/example_certs/Learn how to learn (Google)',\n",
    "    'Business Communication (Google)': '/Users/gilbert/Downloads/example_certs/Business Communication (Google)',\n",
    "    'Get started with Microsoft Teams': '/Users/gilbert/Downloads/example_certs/Get started with Microsoft Teams',\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs': '/Users/gilbert/Downloads/example_certs/Tech for Good: The Role of ICT in Achieving the SDGs',\n",
    "    'Communicate your ideas through storytelling and design (Google)': '/Users/gilbert/Downloads/example_certs/Communicate your ideas through storytelling and design (Google)'\n",
    "    # ... (more certificates and their corresponding folders)\n",
    "}\n",
    "\n",
    "# Populate the cert_name_to_example_paths dictionary by listing all files in each folder\n",
    "cert_name_to_example_paths = {}\n",
    "for cert_name, folder_path in certificate_folders.items():\n",
    "    if os.path.exists(folder_path):\n",
    "        files_in_folder = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        cert_name_to_example_paths[cert_name] = files_in_folder\n",
    "    else:\n",
    "        print(f\"Warning: The folder for '{cert_name}' does not exist at path:\\n\\t{folder_path}\")\n",
    "\n",
    "# Debug: Print to verify\n",
    "print(\"\\n--- Certificate Names and their Example Paths ---\")\n",
    "for cert_name, example_paths in cert_name_to_example_paths.items():\n",
    "    print(f\"\\nCertificate Name: {cert_name}\")\n",
    "    for path in example_paths:\n",
    "        print(f\"\\t- {path}\")\n",
    "\n",
    "# Check if all certificate folders have at least one file\n",
    "print(\"\\n--- Certificate Folders with Missing Example Files ---\")\n",
    "for cert_name, example_paths in cert_name_to_example_paths.items():\n",
    "    if not example_paths:\n",
    "        print(f\"Warning: No example files found for '{cert_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download example_certs to have a record\n",
    "\n",
    "# Create a zip file from the folder\n",
    "shutil.make_archive('/content/example_certs', 'zip', '/content/example_certs')\n",
    "\n",
    "# Download the zip file\n",
    "files.download('/content/example_certs.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342def25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from example certificates to be used for comparrison\n",
    "\n",
    "# Initialize an empty dictionary to hold the extracted text for each example certificate\n",
    "example_cert_texts = {}\n",
    "\n",
    "# Loop through each certificate type and its corresponding example paths\n",
    "for cert_name, example_paths in cert_name_to_example_paths.items():\n",
    "    example_cert_texts[cert_name] = []\n",
    "    print(f\"Processing certificates of type: {cert_name}\")\n",
    "    print(\"=\" * 50)  # Print a separator line for readability\n",
    "\n",
    "    for example_path in example_paths:\n",
    "        try:\n",
    "            if os.path.exists(example_path):\n",
    "                extracted_text = extract_text_from_file(example_path)\n",
    "                example_cert_texts[cert_name].append(extracted_text)\n",
    "                print(f\"Successfully processed: {example_path}\")\n",
    "            else:\n",
    "                print(f\"\\nWarning: The example certificate file for {cert_name} is missing.\\nPath: {example_path}\\n\")\n",
    "                print(\"-\" * 50)  # Print a separator line for readability\n",
    "\n",
    "        except pytesseract.TesseractError as e:\n",
    "            print(f\"\\nError: An issue occurred while processing the file at {example_path}.\\nError Details: {e}\\n\")\n",
    "            print(\"-\" * 50)  # Print a separator line for readability\n",
    "\n",
    "    print(\"=\" * 50)  # Print a separator line for readability\n",
    "\n",
    "# The dictionary example_cert_texts will now hold the extracted text for each certificate that exists\n",
    "print(\"\\nFinal extracted text dictionary:\")\n",
    "\n",
    "for cert_type, extracted_texts in example_cert_texts.items():\n",
    "    print(f\"\\nCertificate Type: {cert_type}\")\n",
    "    print(\"-\" * 50)  # Separator\n",
    "\n",
    "    for i, text in enumerate(extracted_texts):\n",
    "        print(\"\\n\\tExample {}:\\n\\t-------------------\".format(i + 1))\n",
    "        print(\"\\t{}\".format(text.replace('\\n', '\\n\\t')))  # Replacing new lines within the text with new lines followed by tabs for better readability\n",
    "\n",
    "    print(\"=\" * 50)  # Separator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key phrases to calculate the 'Text Similarity Score'\n",
    "\n",
    "# Dictionary to hold key phrases for each certificate type\n",
    "key_phrases_by_cert = {\n",
    "    'Business Communication (Google)': [\n",
    "        'Business Communication', 'Goodwill Community Foundation',\n",
    "        'Congratulations! You answered enough questions correctly',\n",
    "        'Correct Answers: 5/5', 'Total Points: 5/5','Google', 'Passed', 'Assessment Passed', '100%'\n",
    "    ],\n",
    "    'Boost Your Career with SAP Skills My Learning screenshot': [\n",
    "        'Date completed', 'Learning Journey', 'Completed', 'sap', 'SAP Learning Journey', \n",
    "        'Boost Your Career with SAP Skills', 'Available', \"Type Name Status Date completed\", \"Hi, Aliyu - welcome to My Learning\",\n",
    "        \"What's next\"\n",
    "    ],\n",
    "    'Learn how to learn (Google)': [\n",
    "        'Communicate Effectively at Work', 'HAS COMPLETED', 'THIS CERTIFIES THAT', 'AWARDED ON',\n",
    "        'Applied Digital Skills', 'date:', 'lesson:'\n",
    "    ],\n",
    "    'Communicate your ideas through storytelling and design (Google)': [\n",
    "        'Communicate your ideas', 'Storytelling', 'Design', 'Google','OpenClassrooms',\n",
    "        'Congratulations!','Assessment Passed', '4/4 Correct Answers', 'Total Points: 4/4'\n",
    "    ],\n",
    "    'Get started with Microsoft Teams': [\n",
    "        'Microsoft Teams', 'Facilitate meetings', 'and chats', 'through conversations in channels and chats',\n",
    "        'settings as a team owner in Microsoft Teams','Learn how to create teams and channels',\n",
    "        'Personalize your environment by managing your', 'Number of modules completed', \n",
    "        'collaborating with communicate and collaborate more effectively', 'using Outlook with Teams',\n",
    "        'Module title Description Completed Duration', 'Learn how to use Microsoft Teams to schedule'\n",
    "    ],\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs': [\n",
    "        'Tech for Good', 'ICT', 'SDGs', 'Certificate', 'The Role of ICT in Achieving the SDGs',\n",
    "        'Course Progress Dates Discussion Overview About the Partners', \n",
    "        'This represents how much of the course content you have completed',\n",
    "        'You are in an audit track and do not qualify for a certificate', 'Earn a certificate', 'Upgrade now'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CSV path\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/gilbert/Downloads/view_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7df88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv as a dataframe\n",
    "\n",
    "# Read the renamed CSV file into a DataFrame\n",
    "df_learners = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to get an overview of the data\n",
    "df_learners.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b569f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8317831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "df_learners.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c470339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns, including all certificates and screenshots, into a new data frame\n",
    "df_learners_selected = df_learners[['Learner ID',\n",
    "                                    'Email address',\n",
    "                                    'First Name',\n",
    "                                    'Last name',\n",
    "                                    'Block 1 progress %',\n",
    "                                    'Boost Your Career with SAP Skills My Learning screenshot',\n",
    "                                    'Learn how to learn (Google)',\n",
    "                                    'Business Communication (Google)',\n",
    "                                    'Communicate your ideas through storytelling and design (Google)',\n",
    "                                    'Get started with Microsoft Teams',\n",
    "                                    'Tech for Good: The Role of ICT in Achieving the SDGs'\n",
    "                                    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "df_learners_selected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aad4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many learners have started / made progress?\n",
    "\n",
    "df_learners_selected['Block 1 progress %'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only learners who have started\n",
    "df_learners_selected = df_learners_selected[df_learners_selected['Block 1 progress %'] >= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame has been filtered\n",
    "df_learners_selected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values - incomplete courses\n",
    "df_learners_selected.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690aac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract file names of the certificate images uploaded by learners\n",
    "\n",
    "columns_to_process = certificate_names\n",
    "\n",
    "for col in columns_to_process:\n",
    "    for idx, data_str in df_learners_selected[col].items():\n",
    "        if isinstance(data_str, str):  # Skip if not a string\n",
    "            try:\n",
    "                data = literal_eval(data_str)\n",
    "                filenames = []\n",
    "                for entry in data:\n",
    "                    if 'filename' in entry:\n",
    "                        filenames.append(entry['filename'])\n",
    "                if filenames:\n",
    "                    df_learners_selected.at[idx, col] = filenames\n",
    "            except (ValueError, SyntaxError):  # Catching errors related to literal_eval\n",
    "                print(f\"Could not decode: {data_str} in column {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm file names have been extracted in lists\n",
    "df_learners_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_selected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eba990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a specific file\n",
    "\n",
    "# Assuming df_learners_selected is your DataFrame and 'column_name' is the name of the column you want to search in\n",
    "matched_rows = df_learners_selected.loc[df_learners_selected['Boost Your Career with SAP Skills My Learning screenshot'] == 'Screenshot_20230913-233344_Chrome.jpg']\n",
    "\n",
    "# If you want to search in all columns, you can do:\n",
    "matched_rows = df_learners_selected[df_learners_selected.apply(lambda row: row.astype(str).str.contains('Screenshot_20230913-233344_Chrome.jpg').any(), axis=1)]\n",
    "\n",
    "# Now, matched_rows will contain all the rows where 'Screenshot_20230913-233344_Chrome.jpg' appears\n",
    "print(matched_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def split_file_url_complex(entry):\n",
    "    '''\n",
    "    This function, split_file_url_complex, takes an entry from the \"Business Communication (Google)\" column\n",
    "    and separates it into multiple filenames and URLs. It uses regular expressions to find URLs and isolates\n",
    "    the filenames accordingly.\n",
    "\n",
    "    Parameters:\n",
    "        - entry (str): The original entry containing both filenames and URLs.\n",
    "\n",
    "    Returns:\n",
    "        - pd.Series: A pandas Series containing two lists, one for filenames and one for URLs.\n",
    "\n",
    "    Example:\n",
    "        Input: 'Screenshot.png (https://example.com/1), Image.jpg (https://example.com/2)'\n",
    "        Output: pd.Series([['Screenshot.png', 'Image.jpg'], ['https://example.com/1', 'https://example.com/2']])\n",
    "    '''\n",
    "\n",
    "    filenames = []\n",
    "    urls = []\n",
    "\n",
    "    if not isinstance(entry, str):\n",
    "        return pd.Series([filenames, urls])\n",
    "\n",
    "    # Use regex to find all URLs in the entry\n",
    "    found_urls = re.findall(r'https?://[^\\s,)]+', entry)\n",
    "    urls.extend(found_urls)\n",
    "\n",
    "    # Remove the found URLs from the original entry to isolate filenames\n",
    "    for url in found_urls:\n",
    "        entry = entry.replace(url, '')\n",
    "\n",
    "    # Split the remaining entry by comma to get filenames\n",
    "    filenames = entry.split(',')\n",
    "\n",
    "    # Strip extra spaces and parentheses from filenames\n",
    "    filenames = [name.replace('(', '').replace(')', '').strip() for name in filenames]\n",
    "\n",
    "    return pd.Series([filenames, urls])\n",
    "\n",
    "def download_file(url, file_path):\n",
    "    r = requests.get(url)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "def read_pdf_form_fields(file_path):\n",
    "    text_content = ''\n",
    "    pdf_reader = PdfReader(open(file_path, \"rb\"))\n",
    "    for i in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[i]\n",
    "\n",
    "        # Try to read form fields\n",
    "        if '/Annots' in page:\n",
    "            annotations = page['/Annots']\n",
    "            for annotation in annotations:\n",
    "                annotation_object = annotation.get_object()\n",
    "\n",
    "                # Look for form fields\n",
    "                if '/FT' in annotation_object and '/T' in annotation_object:\n",
    "                    field_name = annotation_object['/T']\n",
    "                    field_type = annotation_object['/FT']\n",
    "                    field_value = None\n",
    "\n",
    "                    if field_type == '/Tx':\n",
    "                        field_value = annotation_object.get('/V', None)\n",
    "\n",
    "                    text_content += f\"\\n{field_name}: {field_value}\"\n",
    "    return text_content.strip()\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "    def image_to_text(image_path):\n",
    "        try:\n",
    "            # Try using OpenCV first\n",
    "            print(\"Attempting to read with OpenCV\")\n",
    "            image = cv2.imread(image_path)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            pil_image = Image.fromarray(gray_image)\n",
    "            return pytesseract.image_to_string(pil_image, config='--psm 6').strip()\n",
    "        except Exception as e1:\n",
    "            print(f\"Failed to process {image_path} with OpenCV: {str(e1)}\")\n",
    "            \n",
    "            # Try using PIL as a fallback\n",
    "            try:\n",
    "                print(\"Attempting to read with PIL\")\n",
    "                image = Image.open(image_path)\n",
    "                return pytesseract.image_to_string(image).strip()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with PIL while processing {image_path}: {str(e)}\")\n",
    "                return \"\"\n",
    "\n",
    "    try:\n",
    "        if file_extension == '.pdf':\n",
    "            text_content_plumber = ''\n",
    "\n",
    "            # First, use pdfplumber\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text_content_plumber += page.extract_text()\n",
    "\n",
    "            # Combine and keep unique lines (assuming you have a function read_pdf_form_fields)\n",
    "            text_content_reader = read_pdf_form_fields(file_path)\n",
    "            combined_text = set(text_content_plumber.splitlines()) | set(text_content_reader.splitlines())\n",
    "            combined_text_str = \"\\n\".join(combined_text)\n",
    "\n",
    "            return combined_text_str.strip()\n",
    "\n",
    "        elif file_extension in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "            try:\n",
    "                return image_to_text(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {file_path}: {str(e)}\")\n",
    "                return convert_image_and_process(file_path, image_to_text)\n",
    "\n",
    "        else:\n",
    "            return \"\"  # Unsupported file format\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Test extract_text_from_file(file_path)\n",
    "text = extract_text_from_file(\"/Users/gilbert/Downloads/content/attachments/Boost Your Career with SAP Skills My Learning screenshot/Screenshot_20230913-233344_Chrome.jpg\")\n",
    "print(text)\n",
    "\n",
    "def convert_image_and_process(file_path, processing_function):\n",
    "    try:\n",
    "        file_extension = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "        # If the file is already a PNG, convert to JPG, otherwise convert to PNG\n",
    "        new_extension = '.jpg' if file_extension == '.png' else '.png'\n",
    "\n",
    "        print(f\"Attempting to convert {file_path} to {new_extension} and process again.\")\n",
    "\n",
    "        converted_file_path = file_path.replace(file_extension, new_extension)\n",
    "        image = Image.open(file_path)\n",
    "        image.save(converted_file_path)\n",
    "\n",
    "        return processing_function(converted_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path} even after converting to {new_extension}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def convert_file_to_images(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "    if file_extension not in SUPPORTED_EXTENSIONS:\n",
    "        raise Exception(f\"Unsupported file format: {file_extension}. Manual handling required.\")\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        images = convert_from_path(file_path)\n",
    "    else:\n",
    "        try:\n",
    "            # Try reading with OpenCV first\n",
    "            print(\"Attempting to read with OpenCV\")\n",
    "            img = cv2.imread(file_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(img_rgb)\n",
    "            images.append(pil_img)\n",
    "        except Exception as cv_error:\n",
    "            print(f\"OpenCV failed to process {file_path}: {cv_error}\")\n",
    "            \n",
    "            try:\n",
    "                # Fallback to PIL\n",
    "                images.append(Image.open(file_path))\n",
    "                print(\"Attempting to read with PIL\")\n",
    "            except Exception as pil_error:\n",
    "                print(f\"An error occurred with PIL while processing {file_path}: {pil_error}\")\n",
    "                \n",
    "    return images\n",
    "\n",
    "def compare_images(img1, img2):\n",
    "    img1_gray = cv2.cvtColor(np.array(img1), cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(np.array(img2), cv2.COLOR_BGR2GRAY)\n",
    "# Resize the image if they are not of the same shape\n",
    "    if img1_gray.shape != img2_gray.shape:\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    return ssim(img1_gray, img2_gray)\n",
    "\n",
    "def resize_image(image, scale_percent=50):\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def evaluate_file_similarity(example_file_path, learner_file_path):\n",
    "    # Extract text (Optional, since we are focusing on visual analysis)\n",
    "    example_file_text = extract_text_from_file(example_file_path)\n",
    "    learner_file_text = extract_text_from_file(learner_file_path)\n",
    "\n",
    "    # Convert files to images\n",
    "    example_file_images = convert_file_to_images(example_file_path)\n",
    "    learner_file_images = convert_file_to_images(learner_file_path)\n",
    "\n",
    "    # Assume the first page/image is the most relevant for comparison\n",
    "    example_image_np = np.array(example_file_images[0])\n",
    "    learner_image_np = np.array(learner_file_images[0])\n",
    "\n",
    "    # Resize the images\n",
    "    example_image_resized = resize_image(example_image_np)\n",
    "    learner_image_resized = resize_image(learner_image_np)\n",
    "\n",
    "    # Compare the resized images\n",
    "    similarity_index_resized = compare_images(example_image_resized, learner_image_resized)\n",
    "\n",
    "    return similarity_index_resized\n",
    "\n",
    "def evaluate_batch_similarity(example_file_path, learner_file_paths):\n",
    "    # Store the results here\n",
    "    batch_results = {}\n",
    "\n",
    "    # Convert the example certificate to images\n",
    "    example_file_images = convert_file_to_images(example_file_path)\n",
    "    example_image_np = np.array(example_file_images[0])\n",
    "    example_image_resized = resize_image(example_image_np)\n",
    "\n",
    "    for learner_file_path in learner_file_paths:\n",
    "        # Convert learner files to images\n",
    "        learner_file_images = convert_file_to_images(learner_file_path)\n",
    "        learner_image_np = np.array(learner_file_images[0])\n",
    "        learner_image_resized = resize_image(learner_image_np)\n",
    "\n",
    "        # Compare the resized images\n",
    "        similarity_index_resized = compare_images(example_image_resized, learner_image_resized)\n",
    "\n",
    "        # Store the result\n",
    "        batch_results[learner_file_path] = similarity_index_resized\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def calculate_text_similarity(reference_text, comparison_text):\n",
    "    if not comparison_text:\n",
    "        return 0\n",
    "    return fuzz.ratio(reference_text, comparison_text)\n",
    "\n",
    "def calculate_text_similarity_key_phrases(reference_key_phrases, comparison_text):\n",
    "    if not comparison_text:\n",
    "        return 0\n",
    "\n",
    "    score = 0\n",
    "    for phrase in reference_key_phrases:\n",
    "        if phrase.lower() in comparison_text.lower():  # Case insensitive check\n",
    "            score += 1\n",
    "\n",
    "    # Normalize by the total number of key phrases to get a similarity score between 0 and 100\n",
    "    normalized_score = (score / len(reference_key_phrases)) * 100\n",
    "\n",
    "    return normalized_score\n",
    "\n",
    "def calculate_learner_id_similarity(comparison_text, learner_id):\n",
    "    # Tokenize both strings into words\n",
    "    comparison_words = set(comparison_text.lower().split())\n",
    "    learner_id_words = set(learner_id.lower().split())\n",
    "\n",
    "    # Calculate intersection and union of both sets\n",
    "    intersection = len(comparison_words & learner_id_words)\n",
    "    union = len(comparison_words | learner_id_words)\n",
    "\n",
    "    # If union is zero, return 0% similarity\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate Jaccard similarity and multiply by 100 for percentage\n",
    "    similarity = (intersection / union) * 100\n",
    "\n",
    "    return round(similarity, 2)  # Round to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a string cert_data_str which can be either a list-like object or a JSON-like string. It processes it to return a list.\n",
    "\n",
    "def process_cert_data(cert_data_str):\n",
    "    if isinstance(cert_data_str, list):\n",
    "        if not cert_data_str:  # Skip if the list is empty\n",
    "            return []\n",
    "        return cert_data_str  # It's already a list\n",
    "    else:\n",
    "        if pd.isna(cert_data_str) or not cert_data_str:  # Check if it's NaN or an empty string\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Parsing the string into a list of dictionaries\n",
    "            return json.loads(cert_data_str.replace(\"'\", '\"'))\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function looks into cache_df DataFrame to find if a cached similarity exists for a given cache_key. If it does, it returns those values; otherwise, it calculates new ones and updates the cache.\n",
    "\n",
    "def fetch_cached_similarity(cache_df, cache_key, example_cert_path, file_path, learner_id):\n",
    "    cached_row = cache_df[cache_df['Cache Key'] == cache_key]\n",
    "    \n",
    "    if cached_row.empty:\n",
    "        # Assuming functions evaluate_file_similarity, calculate_text_similarity,\n",
    "        # and calculate_learner_id_similarity are defined elsewhere\n",
    "        visual_similarity = evaluate_file_similarity(example_cert_path, file_path)\n",
    "        text_similarity = calculate_text_similarity(example_cert_path, file_path)\n",
    "        learner_id_similarity = calculate_learner_id_similarity(learner_id, file_path)\n",
    "\n",
    "        new_cache_row = pd.DataFrame({\n",
    "            'Cache Key': [cache_key],\n",
    "            'Visual Similarity': [visual_similarity],\n",
    "            'Text Similarity': [text_similarity],\n",
    "            'Learner ID Similarity': [learner_id_similarity]\n",
    "        })\n",
    "\n",
    "        # Check for NA entries before concatenating\n",
    "        if new_cache_row.dropna().empty:\n",
    "            print(\"new_cache_row contains only NA entries; skipping concatenation.\")\n",
    "        else:\n",
    "            cache_df = pd.concat([cache_df, new_cache_row], ignore_index=True)\n",
    "\n",
    "        return visual_similarity, text_similarity, learner_id_similarity, cache_df\n",
    "    else:\n",
    "        visual_similarity = cached_row['Visual Similarity'].iloc[0]\n",
    "        text_similarity = cached_row['Text Similarity'].iloc[0]\n",
    "        learner_id_similarity = cached_row['Learner ID Similarity'].iloc[0]\n",
    "\n",
    "        return visual_similarity, text_similarity, learner_id_similarity, cache_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean file paths\n",
    "def sanitize_path(file_path):\n",
    "    return os.path.normpath(file_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# Check file paths exist\n",
    "def check_file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "# Log Errors with extra info\n",
    "def log_error(learner_id, certificate_column, file_name, error_msg, extra_info=None):\n",
    "    new_row = pd.DataFrame({\n",
    "        'Learner ID': [learner_id],\n",
    "        'Certificate Type': [certificate_column],\n",
    "        'File Name': [file_name],\n",
    "        'Status': ['Failure'],\n",
    "        'Highest Visual Similarity': [None],\n",
    "        'Text Similarity Score': [None],\n",
    "        'Learner ID Similarity Score': [None],\n",
    "        'Errors': [error_msg],\n",
    "        'Extra Info': [extra_info]  # Add this line\n",
    "    })\n",
    "    return new_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save log DataFrame to CSV\n",
    "def save_log_to_csv(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "# Function to load log DataFrame from CSV\n",
    "def load_log_from_csv(path):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['Learner ID', 'Certificate Type', 'File Name', 'Status', 'Highest Visual Similarity', 'Text Similarity Score', 'Learner ID Similarity Score', 'Errors', 'Extra Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd941b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop to evaluate the learners uploaded images against the example certificates\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load existing log_df from CSV if it exists\n",
    "    log_csv_path = '/Users/gilbert/Downloads/log_df.csv'\n",
    "    log_df = load_log_from_csv(log_csv_path)\n",
    "\n",
    "    processed_entries = set(log_df.apply(lambda row: (row['Learner ID'], row['Certificate Type'], row['File Name']), axis=1))\n",
    "    \n",
    "    row_limit = 10000  # Define the row limit for testing\n",
    "\n",
    "    row_counter = 0  # Initialize a counter\n",
    "\n",
    "    columns_to_process = certificate_names  # Columns to process\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        if row_counter >= row_limit:\n",
    "            print(f\"Reached row limit of {row_limit}. Stopping processing.\")\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "\n",
    "        log_rows = []  # Initialize an empty list to collect new log rows for this learner\n",
    "\n",
    "        for certificate_column in columns_to_process:\n",
    "\n",
    "            cert_data_list = row[certificate_column]\n",
    "\n",
    "            if not isinstance(cert_data_list, list):\n",
    "                continue\n",
    "\n",
    "            for file_name in cert_data_list:\n",
    "\n",
    "                entry = (learner_id, certificate_column, file_name)\n",
    "                \n",
    "                if entry in processed_entries:\n",
    "                    print(f\"Skipping already processed entry: {entry}\")\n",
    "                    continue\n",
    "                \n",
    "                file_path = os.path.join(attachments_dir, certificate_column, file_name)\n",
    "                file_path = sanitize_path(file_path)  # Sanitize the path\n",
    "\n",
    "                if not check_file_exists(file_path):  # Check if file exists\n",
    "                    new_row = log_error(learner_id, certificate_column, file_name, f\"File not found: {file_path}\").iloc[0].to_dict()\n",
    "                    log_rows.append(new_row)\n",
    "                    continue  # Skip this file and move to the next\n",
    "\n",
    "                example_cert_paths = cert_name_to_example_paths.get(certificate_column, [])\n",
    "\n",
    "                for example_cert_path in example_cert_paths:\n",
    "\n",
    "                    try:\n",
    "                        cache_key = f\"{file_path}-{example_cert_path}\"\n",
    "                        visual_similarity, text_similarity, learner_id_similarity, cache_df = fetch_cached_similarity(\n",
    "                            cache_df, cache_key, example_cert_path, file_path, learner_id)\n",
    "\n",
    "                        new_row = {\n",
    "                            'Learner ID': learner_id,\n",
    "                            'Certificate Type': certificate_column,\n",
    "                            'File Name': file_name,\n",
    "                            'Status': 'Success',\n",
    "                            'Highest Visual Similarity': visual_similarity,\n",
    "                            'Text Similarity Score': text_similarity,\n",
    "                            'Learner ID Similarity Score': learner_id_similarity,\n",
    "                            'Errors': None  # No errors, so None\n",
    "                        }\n",
    "                        log_rows.append(new_row)\n",
    "\n",
    "                    except FileNotFoundError:\n",
    "                        error_msg = f\"File not found: {file_path} or {example_cert_path}\"\n",
    "                        extra_info = f\"Current working directory: {os.getcwd()}, File Size: {os.path.getsize(file_path) if os.path.exists(file_path) else 'N/A'}\"\n",
    "                        new_row = log_error(learner_id, certificate_column, file_name, error_msg, extra_info).iloc[0].to_dict()\n",
    "                        log_rows.append(new_row)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"An exception occurred: {e}\"\n",
    "                        extra_info = f\"Exception type: {type(e).__name__}, Arguments: {e.args}\"\n",
    "                        new_row = log_error(learner_id, certificate_column, file_name, error_msg, extra_info).iloc[0].to_dict()\n",
    "                        log_rows.append(new_row)\n",
    "\n",
    "                # Increment the counter at the end of processing each row\n",
    "                row_counter += 1\n",
    "\n",
    "        # Save log_df to CSV after each learner is processed\n",
    "        log_df = pd.concat([log_df, pd.DataFrame(log_rows)], ignore_index=True)\n",
    "        save_log_to_csv(log_df, log_csv_path)\n",
    "\n",
    "        # Add the processed learner to the set\n",
    "        processed_entries.add(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ba127",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['Learner ID Similarity Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df[log_df['Learner ID Similarity Score']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1217fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save log_df as csv\n",
    "log_df.to_csv('/Users/gilbert/Downloads/log_df_backup2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload log_df if you want to use a saved copy. Usually you would generate it in the Main Function.\n",
    "# log_df = pd.read_csv('/Users/gilbert/Downloads/log_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61091a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to avoid truncation of text in the Errors column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now print the 'Errors' column\n",
    "log_df[log_df['Errors'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the error files for inspection - FILES COPIED SEEM FINE? I think there an error in Errors logging\n",
    "\n",
    "# Create error directory if it doesn't exist\n",
    "error_dir = os.path.join(attachments_dir, 'errors')\n",
    "if not os.path.exists(error_dir):\n",
    "    os.makedirs(error_dir)\n",
    "\n",
    "# Copy files that produced errors\n",
    "for idx, row in log_df[log_df['Errors'].notna()].iterrows():\n",
    "    src_path = os.path.join(attachments_dir, row['Certificate Type'], row['File Name'])\n",
    "    dest_file_name = f\"{row['Learner ID']} - {row['Certificate Type']} - {row['File Name']}\"\n",
    "    dest_path = os.path.join(error_dir, dest_file_name)\n",
    "\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Text Similarity Score' to numeric, setting errors='coerce' will turn unconvertible values to NaN\n",
    "log_df['Text Similarity Score'] = pd.to_numeric(log_df['Text Similarity Score'], errors='coerce')\n",
    "\n",
    "# Convert 'Learner ID Similarity Score' to numeric, setting errors='coerce' will turn unconvertible values to NaN\n",
    "log_df['Learner ID Similarity Score'] = pd.to_numeric(log_df['Learner ID Similarity Score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d45bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scores\n",
    "\n",
    "# Columns to analyze\n",
    "columns_to_analyze = ['Highest Visual Similarity', 'Text Similarity Score', 'Learner ID Similarity Score']\n",
    "\n",
    "for col in columns_to_analyze:\n",
    "    filled_df = log_df.fillna({col: 0})\n",
    "    global_min = filled_df[col].min()\n",
    "    global_max = filled_df[col].max()\n",
    "\n",
    "    # Create the grid of subplots\n",
    "    g = sns.FacetGrid(log_df, col='Certificate Type', col_wrap=3, height=4, aspect=1.2)\n",
    "    g = (g.map(plt.hist, col, bins=20, edgecolor='black', range=[global_min, global_max])\n",
    "         .set_axis_labels(col, 'Frequency')\n",
    "         .set_titles(\"Certificate Type: {col_name}\"))\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    g.fig.subplots_adjust(top=0.9, bottom=0.1, wspace=0.2, hspace=0.4)\n",
    "    g.fig.suptitle(f'{col} by Certificate Type', fontsize=16)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37342ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df[['Highest Visual Similarity', 'Text Similarity Score', 'Learner ID Similarity Score']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df[log_df['Status'] != 'Success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df[log_df['Status'] != 'Success']['Errors'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a680d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot data\n",
    "\n",
    "# Group by 'Learner ID' and 'Certificate Type' and take the max value for each group\n",
    "grouped_df = log_df[['Learner ID', 'Certificate Type', 'Highest Visual Similarity', 'Text Similarity Score', 'Learner ID Similarity Score']].groupby(['Learner ID', 'Certificate Type']).max().reset_index()\n",
    "\n",
    "# Reshape the DataFrame\n",
    "pivot_df = pd.pivot_table(grouped_df, index='Learner ID', columns='Certificate Type',\n",
    "                          values=['Highest Visual Similarity', 'Text Similarity Score', 'Learner ID Similarity Score'],\n",
    "                          aggfunc='first')\n",
    "\n",
    "# Flatten the multi-level column names and add custom label\n",
    "pivot_df.columns = [f\"{col[1]}_{col[0]}\" for col in pivot_df.columns.values]\n",
    "\n",
    "# Reset index\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Sort columns\n",
    "sorted_cols = ['Learner ID'] + sorted([col for col in pivot_df.columns if col != 'Learner ID'])\n",
    "pivot_df = pivot_df[sorted_cols]\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV\n",
    "pivot_df.to_csv(\"pivot_backup2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97be1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thresholds for accepting / rejecting certificates\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "calculated_thresholds_df = pd.DataFrame(columns=['Certificate Type', 'Visual_Threshold_Percentile', 'Visual_Threshold_Zscore', 'Text_Threshold_Percentile', 'Text_Threshold_Zscore', 'Learner_ID_Threshold_Percentile', 'Learner_ID_Threshold_Zscore'])\n",
    "\n",
    "# Loop through each certificate type to calculate thresholds\n",
    "for cert_type in certificate_names:\n",
    "    visual_data = pivot_df[f\"{cert_type}_Highest Visual Similarity\"].dropna()\n",
    "    text_data = pivot_df[f\"{cert_type}_Text Similarity Score\"].dropna()\n",
    "    learner_id_data = pivot_df[f\"{cert_type}_Learner ID Similarity Score\"].dropna()\n",
    "\n",
    "    # Calculate Percentile\n",
    "    visual_threshold_percentile = visual_data.quantile(percentile_factor)\n",
    "    text_threshold_percentile = text_data.quantile(percentile_factor)\n",
    "    learner_id_threshold_percentile = learner_id_data.quantile(percentile_factor)\n",
    "\n",
    "    # Calculate Z-score thresholds\n",
    "    visual_threshold_zscore = visual_data.mean() + z_score_factor * visual_data.std()\n",
    "    text_threshold_zscore = text_data.mean() + z_score_factor * text_data.std()\n",
    "    learner_id_threshold_zscore = learner_id_data.mean() + z_score_factor * learner_id_data.std()\n",
    "\n",
    "    # Create a DataFrame for the current certificate type\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Certificate Type': [cert_type],\n",
    "        'Visual_Threshold_Percentile': [visual_threshold_percentile],\n",
    "        'Visual_Threshold_Zscore': [visual_threshold_zscore],\n",
    "        'Text_Threshold_Percentile': [text_threshold_percentile],\n",
    "        'Text_Threshold_Zscore': [text_threshold_zscore],\n",
    "        'Learner_ID_Threshold_Percentile': [learner_id_threshold_percentile],\n",
    "        'Learner_ID_Threshold_Zscore': [learner_id_threshold_zscore]\n",
    "    })\n",
    "\n",
    "    # Concatenate to the existing DataFrame\n",
    "    calculated_thresholds_df = pd.concat([calculated_thresholds_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Show the DataFrame with calculated thresholds\n",
    "calculated_thresholds_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store dynamic thresholds\n",
    "thresholds = {}\n",
    "\n",
    "# Loop through the DataFrame to populate the thresholds dictionary\n",
    "for index, row in calculated_thresholds_df.iterrows():\n",
    "    cert_type = row['Certificate Type']\n",
    "\n",
    "    thresholds[cert_type] = {\n",
    "        'visual': row['Visual_Threshold_Percentile'],  # Replace with row['Visual_Threshold_Zscore'] if you want to use Z-score\n",
    "        'text': row['Text_Threshold_Percentile'],  # Replace with row['Text_Threshold_Zscore'] if you want to use Z-score\n",
    "        'learner_id': row['Learner_ID_Threshold_Percentile']  # Replace with row['Learner_ID_Threshold_Zscore'] if you want to use Z-score\n",
    "    }\n",
    "\n",
    "# Now the thresholds dictionary is dynamically populated based on calculated_thresholds_df\n",
    "thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually input thresholds\n",
    "thresholds = {\n",
    "'Boost Your Career with SAP Skills My Learning screenshot': {'visual': 0.50,'text': 50.0,'learner_id': 0.0},\n",
    "'Learn how to learn (Google)': {'visual': 0.50,'text': 50.0,'learner_id': 0.0},\n",
    "'Business Communication (Google)': {'visual': 0.50,'text': 50.0,'learner_id': 0.0},\n",
    "'Communicate your ideas through storytelling and design (Google)': {'visual': 0.50,'text': 50.0, 'learner_id': 0.0},\n",
    "'Get started with Microsoft Teams': {'visual': 0.50,'text': 50.0,'learner_id': 0.0},\n",
    "'Tech for Good: The Role of ICT in Achieving the SDGs': {'visual': 0.45,'text': 50.0,'learner_id': 0.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate images based on thresholds\n",
    "\n",
    "# Create new columns based on certificate-specific thresholds and reasons for rejection\n",
    "for cert_type in thresholds.keys():\n",
    "    visual_threshold = thresholds[cert_type]['visual']\n",
    "    text_threshold = thresholds[cert_type]['text']\n",
    "    learner_id_threshold = thresholds[cert_type]['learner_id']\n",
    "\n",
    "    col_suffix = f\"{cert_type}_AcceptOrReject\"\n",
    "    reason_suffix = f\"{cert_type}_RejectionReasons\"\n",
    "\n",
    "    # Create a series with rejection reasons\n",
    "    def find_rejection_reasons(row):\n",
    "        reasons = []\n",
    "        if row.get(f\"{cert_type}_Highest Visual Similarity\", 0) < visual_threshold:\n",
    "            reasons.append('Visual')\n",
    "        if row.get(f\"{cert_type}_Text Similarity Score\", 0) < text_threshold:\n",
    "            reasons.append('Text')\n",
    "        if row.get(f\"{cert_type}_Learner ID Similarity Score\", 0) < learner_id_threshold:\n",
    "            reasons.append('ID')\n",
    "        return reasons\n",
    "\n",
    "    # Create the AcceptOrReject and RejectionReasons columns\n",
    "    pivot_df.loc[:, col_suffix] = (\n",
    "        (pivot_df.get(f\"{cert_type}_Highest Visual Similarity\", 0) >= visual_threshold) &\n",
    "        (pivot_df.get(f\"{cert_type}_Text Similarity Score\", 0) >= text_threshold) &\n",
    "        (pivot_df.get(f\"{cert_type}_Learner ID Similarity Score\", 0) >= learner_id_threshold)\n",
    "    ).astype(int)\n",
    "\n",
    "    pivot_df[reason_suffix] = pivot_df.apply(find_rejection_reasons, axis=1)\n",
    "\n",
    "# Initialize an empty list to store the new order of columns\n",
    "new_column_order = ['Learner ID']  # Start with 'Learner ID'\n",
    "\n",
    "# Loop through each certificate type and arrange the columns\n",
    "for cert_type in thresholds.keys():\n",
    "    new_column_order.extend([\n",
    "        f\"{cert_type}_Highest Visual Similarity\",\n",
    "        f\"{cert_type}_Text Similarity Score\",\n",
    "        f\"{cert_type}_Learner ID Similarity Score\",\n",
    "        f\"{cert_type}_AcceptOrReject\",\n",
    "        f\"{cert_type}_RejectionReasons\"  # Include the new RejectionReasons column\n",
    "    ])\n",
    "\n",
    "# Reorder the DataFrame columns based on the new list\n",
    "pivot_df = pivot_df[new_column_order]\n",
    "\n",
    "# Show the DataFrame with reordered columns\n",
    "pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all columns that contain \"AcceptOrReject\"\n",
    "accept_or_reject_cols = [col for col in pivot_df.columns if \"AcceptOrReject\" in col]\n",
    "\n",
    "# Compute the minimum value along the rows for the AcceptOrReject columns\n",
    "minimums = pivot_df[accept_or_reject_cols].min(axis=1)\n",
    "\n",
    "# Create the 'Learner Status' column\n",
    "# If minimum is 0, set 'Learner Status' to 0, else set to 1\n",
    "pivot_df['Learner Status'] = (minimums != 0).astype(int)\n",
    "\n",
    "# View the updated DataFrame\n",
    "pivot_df[['Learner ID','Learner Status'] + accept_or_reject_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an explanation by identifying which certificates were rejected\n",
    "\n",
    "# Extract all columns that contain \"AcceptOrReject\"\n",
    "accept_or_reject_cols = [col for col in pivot_df.columns if \"AcceptOrReject\" in col]\n",
    "\n",
    "# Compute the minimum value along the rows for the AcceptOrReject columns\n",
    "minimums = pivot_df[accept_or_reject_cols].min(axis=1)\n",
    "\n",
    "# Create the 'Learner Status' column\n",
    "# If minimum is 0, set 'Learner Status' to 0, else set to 1\n",
    "pivot_df['Learner Status'] = (minimums != 0).astype(int)\n",
    "\n",
    "# Create a new column to store the names of the AcceptOrReject columns that have 0 values\n",
    "def find_zero_cols(row):\n",
    "    zero_cols = [col.replace(\"_AcceptOrReject\", \"\") for col in accept_or_reject_cols if row[col] == 0]\n",
    "    return zero_cols  # Returning a list instead of a string\n",
    "\n",
    "pivot_df['Rejected Certificates'] = pivot_df.apply(find_zero_cols, axis=1)\n",
    "\n",
    "# View the updated DataFrame\n",
    "pivot_df[['Learner ID', 'Learner Status', 'Rejected Certificates'] + accept_or_reject_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df[pivot_df['Learner Status']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00979440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df[pivot_df['Learner Status']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ecd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pivot_df and df_learners_selected to compare scores with screenshots\n",
    "\n",
    "# Merge pivot_df and df_learners_selected based on 'Learner ID', keeping only the rows in pivot_df\n",
    "df_learners_evaluated = pd.merge(pivot_df, df_learners_selected, on='Learner ID', how='left')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_learners_evaluated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_evaluated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_evaluated.to_csv('/Users/gilbert/Downloads/learners_evaluated_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841833ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect failed learners with histograms\n",
    "\n",
    "# Filter the DataFrame where 'Learner Status' equals 0\n",
    "filtered_df = df_learners_evaluated[\n",
    "    (df_learners_evaluated['Learner Status'] == 0) &\n",
    "    (df_learners_evaluated['Rejected Certificates'] == 'Tech for Good: The Role of ICT in Achieving the SDGs')\n",
    "]\n",
    "\n",
    "# Columns to create histograms for\n",
    "columns_to_plot = [\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs_Highest Visual Similarity',\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs_Text Similarity Score',\n",
    "    'Tech for Good: The Role of ICT in Achieving the SDGs_Learner ID Similarity Score'\n",
    "]\n",
    "\n",
    "# Create histograms\n",
    "for col in columns_to_plot:\n",
    "    sns.histplot(filtered_df[col], kde=False, bins=30)  # You can customize bins and other style options here\n",
    "    plt.title(f\"Histogram for {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_learners_evaluated[df_learners_evaluated['Learner Status'] == 0]['Rejected Certificates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4009207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_evaluated[(df_learners_evaluated['Learner Status'] == 0) & df_learners_evaluated['Rejected Certificates'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cf6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve learner screenshots to inspect by cert type\n",
    "\n",
    "import math\n",
    "\n",
    "destination_base_folder = '/Users/gilbert/Downloads/check_temp'\n",
    "\n",
    "if not os.path.exists(destination_base_folder):\n",
    "    os.makedirs(destination_base_folder)\n",
    "\n",
    "for index, row in df_learners_evaluated.iterrows():\n",
    "    if row['Learner Status'] == 0:\n",
    "        learner_id = row['Learner ID']\n",
    "        rejected_certs = row['Rejected Certificates']\n",
    "        \n",
    "        for cert_col in rejected_certs:\n",
    "            destination_folder = os.path.join(destination_base_folder, cert_col)\n",
    "            if not os.path.exists(destination_folder):\n",
    "                os.makedirs(destination_folder)\n",
    "\n",
    "            cert_names = row[cert_col]\n",
    "            if not isinstance(cert_names, list):\n",
    "                cert_names = [certificate_names]\n",
    "\n",
    "                print(f\"cert_names before loop: {cert_names}\")\n",
    "\n",
    "                for cert_name in cert_names:\n",
    "\n",
    "                    if cert_name is None or (isinstance(cert_name, float) and math.isnan(cert_name)):\n",
    "                        print(f\"Skipping invalid cert_name: {cert_name}\")\n",
    "                        continue\n",
    "        \n",
    "                    source_folder = f\"/Users/gilbert/Downloads/content/attachments/{cert_col}\"\n",
    "                    source_file_path = os.path.join(source_folder, str(cert_name))  # Convert to str just in case\n",
    "\n",
    "                    print(f\"Checking existence of {source_file_path}\")  # Debug print\n",
    "                    \n",
    "                    if os.path.exists(source_file_path):\n",
    "                        file_extension = os.path.splitext(cert_name)[1]\n",
    "                        new_file_name = f\"{learner_id}{file_extension}\"\n",
    "                        destination_file_path = os.path.join(destination_folder, new_file_name)\n",
    "    \n",
    "                        print(f\"Copying {source_file_path} to {destination_file_path}\")  # Debug print\n",
    "    \n",
    "                        shutil.copy(source_file_path, destination_file_path)\n",
    "                    else:\n",
    "                        print(f\"File {source_file_path} does not exist.\")  # Debug print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa154e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve learner screenshots to inspect - by learner\n",
    "destination_base_folder = '/Users/gilbert/Downloads/check_temp'\n",
    "\n",
    "if not os.path.exists(destination_base_folder):\n",
    "    os.makedirs(destination_base_folder)\n",
    "\n",
    "for index, row in df_learners_evaluated.iterrows():\n",
    "    if row['Learner Status'] == 0:\n",
    "        learner_id = row['Learner ID']\n",
    "        folder_name = learner_id\n",
    "\n",
    "        destination_folder = os.path.join(destination_base_folder, folder_name)\n",
    "        if not os.path.exists(destination_folder):\n",
    "            os.makedirs(destination_folder)\n",
    "\n",
    "        rejected_certs = row['Rejected Certificates']\n",
    "        \n",
    "        for cert_col in rejected_certs:\n",
    "            cert_names = row[cert_col]\n",
    "            if not isinstance(cert_names, list):\n",
    "                cert_names = [cert_names]\n",
    "\n",
    "            for cert_name in cert_names:\n",
    "                source_folder = f\"/Users/gilbert/Downloads/content/attachments/{cert_col}\"\n",
    "                source_file_path = os.path.join(source_folder, cert_name)\n",
    "\n",
    "                print(f\"Checking existence of {source_file_path}\")  # Debug print\n",
    "                \n",
    "                if os.path.exists(source_file_path):\n",
    "                    file_extension = os.path.splitext(cert_name)[1]\n",
    "                    new_file_name = f\"{cert_col}{file_extension}\"\n",
    "                    destination_file_path = os.path.join(destination_folder, new_file_name)\n",
    "\n",
    "                    print(f\"Copying {source_file_path} to {destination_file_path}\")  # Debug print\n",
    "\n",
    "                    shutil.copy(source_file_path, destination_file_path)\n",
    "                else:\n",
    "                    print(f\"File {source_file_path} does not exist.\")  # Debug print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94411967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_learners_evaluated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a learner\n",
    "\n",
    "# Select a specific learner based on 'Learner ID'\n",
    "specific_learner_df = df_learners_evaluated[df_learners_evaluated['Learner ID'] == 'Olawale  Raheem  - olawalehafees13@gmail.com']\n",
    "\n",
    "# Identify columns that end with '_RejectionReasons'\n",
    "rejection_reason_cols = [col for col in df_learners_evaluated.columns if col.endswith('_RejectionReasons')]\n",
    "\n",
    "# Create a list with 'Rejected Certificates' and all columns ending with '_RejectionReasons'\n",
    "columns_to_show = ['Rejected Certificates'] + rejection_reason_cols\n",
    "\n",
    "# Filter the DataFrame based on these columns\n",
    "filtered_learner_df = specific_learner_df[columns_to_show]\n",
    "\n",
    "# Display the result\n",
    "filtered_learner_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4333359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file from the folder\n",
    "shutil.make_archive('/content/check_temp', 'zip', '/content/check_temp')\n",
    "\n",
    "# Trigger the download\n",
    "files.download('/content/check_temp.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9310d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload or specify the path to the new example certificate\n",
    "new_example_cert_path = '/content/temp_downloads/Ajibade_Business_Communication.PNG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c75a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a batch of XX rows from final_log_df\n",
    "batch_log_df = final_log_df.head(100).copy()\n",
    "\n",
    "# Initialize a new column for the visual similarity to the new example\n",
    "# and another for the file paths\n",
    "batch_log_df['New_Example_Visual_Similarity'] = None\n",
    "batch_log_df['File Path'] = None  # New column for file paths\n",
    "\n",
    "for idx, row in batch_log_df.iterrows():\n",
    "    learner_id = row['Learner ID']\n",
    "    file_name = row['File Name']\n",
    "    learner_file_path = os.path.join(\"temp_downloads\", file_name)  # Assuming files are in temp_downloads\n",
    "\n",
    "    # Populate the File Path column\n",
    "    batch_log_df.loc[idx, 'File Path'] = learner_file_path\n",
    "\n",
    "    try:\n",
    "        # Evaluate similarity using the new example certificate\n",
    "        similarity_score = evaluate_file_similarity(new_example_cert_path, learner_file_path)\n",
    "\n",
    "        # Update the DataFrame with the new similarity score\n",
    "        batch_log_df.loc[idx, 'New_Example_Visual_Similarity'] = similarity_score\n",
    "\n",
    "        # Optionally log success\n",
    "        print(f\"Processed Learner ID: {learner_id}, File: {file_name}\")\n",
    "    except Exception as e:\n",
    "        # Log the exception\n",
    "        batch_log_df.loc[idx, 'New_Example_Visual_Similarity'] = f\"Failed - {str(e)}\"\n",
    "        print(f\"Failed for Learner ID: {learner_id}, File: {file_name}. Reason: {str(e)}\")\n",
    "\n",
    "# Convert the file paths into clickable URLs\n",
    "batch_log_df['File URL'] = 'file://' + batch_log_df['File Path'].astype(str)\n",
    "\n",
    "# Merge the batch log back into the final log\n",
    "# final_log_df = pd.merge(final_log_df, batch_log_df, on=['Learner ID', 'File Name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_log_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_log_df['Status'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_log_df['Highest Visual Similarity'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2423624",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_log_df['New_Example_Visual_Similarity'] = pd.to_numeric(batch_log_df['New_Example_Visual_Similarity'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f545796",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_log_df['New_Example_Visual_Similarity'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV\n",
    "batch_log_df.to_csv(\"batch_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c63154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_histogram(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "def compare_histograms(hist1, hist2):\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd7738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# Function to evaluate the similarity between two images\n",
    "def evaluate_file_similarity(example_file_path, learner_file_path):\n",
    "    example_hist = calculate_histogram(example_file_path)\n",
    "    learner_hist = calculate_histogram(learner_file_path)\n",
    "    similarity = compare_histograms(example_hist, learner_hist)\n",
    "    return similarity\n",
    "\n",
    "# Function to find the local winner from a group of images\n",
    "def find_local_winner(image_group):\n",
    "    max_similarity = 0\n",
    "    winner = None\n",
    "\n",
    "    for img1, img2 in combinations(image_group, 2):\n",
    "        similarity = evaluate_file_similarity(img1, img2)\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            winner = img1  # or img2 depending on your criteria\n",
    "\n",
    "    return winner\n",
    "\n",
    "# Function to find the global winner from local winners\n",
    "def find_global_winner(local_winners):\n",
    "    return find_local_winner(local_winners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tournament_runtime(folder_path, group_size):\n",
    "    # Get the number of image files in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    # Calculate the number of groups\n",
    "    num_groups = (num_images + group_size - 1) // group_size  # Ceiling division\n",
    "\n",
    "    # Estimate the number of comparisons for local winners\n",
    "    local_comparisons = (group_size * (group_size - 1)) // 2\n",
    "    total_local_comparisons = local_comparisons * num_groups\n",
    "\n",
    "    # Estimate the number of comparisons for the global winner\n",
    "    global_comparisons = (num_groups * (num_groups - 1)) // 2\n",
    "\n",
    "    # Total number of comparisons\n",
    "    total_comparisons = total_local_comparisons + global_comparisons\n",
    "\n",
    "    # Estimate the size of one image (use the first image as a sample)\n",
    "    sample_image_path = os.path.join(folder_path, image_files[0])\n",
    "    sample_image_size = os.path.getsize(sample_image_path)  # in bytes\n",
    "\n",
    "    # Estimate total size for all images\n",
    "    total_size = sample_image_size * num_images  # in bytes\n",
    "\n",
    "    # Estimate runtime (this is a very rough estimate; actual time can vary)\n",
    "    # Assume 0.001 second per comparison per 1000 bytes (this is a made-up number for demonstration)\n",
    "    estimated_time = 0.001 * total_comparisons * (total_size / 1000)  # in seconds\n",
    "\n",
    "    return estimated_time, num_images, total_comparisons, total_size\n",
    "\n",
    "# Usage\n",
    "folder_path = \"/content/temp_downloads\"\n",
    "group_size = 50\n",
    "estimated_time, num_images, total_comparisons, total_size = estimate_tournament_runtime(folder_path, group_size)\n",
    "print(f\"Estimated time: {estimated_time} seconds\")\n",
    "print(f\"Number of images: {num_images}\")\n",
    "print(f\"Total number of comparisons: {total_comparisons}\")\n",
    "print(f\"Total size: {total_size} bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e02748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all image files in the folder\n",
    "all_image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "group_size = 15  # or any other number depending on your needs\n",
    "\n",
    "# Step 1: Divide into groups\n",
    "image_groups = [all_image_paths[i:i + group_size] for i in range(0, len(all_image_paths), group_size)]\n",
    "\n",
    "# Step 2: Find local winners (can be parallelized)\n",
    "local_winners = [find_local_winner(group) for group in image_groups]\n",
    "\n",
    "# Step 3: Find global winner\n",
    "global_winner = find_global_winner(local_winners)\n",
    "\n",
    "print(f\"The most visually similar image is: {global_winner}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to keep the \"_x\" columns and drop the \"_y\" columns\n",
    "cols_to_keep = [col for col in final_log_df.columns if not col.endswith('_y')]\n",
    "\n",
    "# Remove \"_x\" suffix from column names\n",
    "final_log_df = final_log_df[cols_to_keep]\n",
    "final_log_df.columns = [col.replace('_x', '') for col in cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all columns ending with \"_y\"\n",
    "cols_to_drop = [col for col in final_log_df.columns if col.endswith('_y')]\n",
    "final_log_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Removing \"_x\" suffix from column names\n",
    "final_log_df.columns = [col.replace('_x', '') for col in final_log_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2622565",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_log_df.drop(columns='New_Example_Visual_Similarity', inplace=True)\n",
    "final_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths with the actual file paths\n",
    "new_example_cert_path = \"new_example_cert.png\"\n",
    "learner_cert_path = \"/content/temp_downloads/Ajibade_Business_Communication.PNG\"\n",
    "\n",
    "# Manually evaluate file similarity\n",
    "similarity_score = evaluate_file_similarity(new_example_cert_path, learner_cert_path)\n",
    "\n",
    "print(f\"The visual similarity score between the two files is: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4e0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD Main Function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    columns_to_process = [\n",
    "        'Boost Your Career with SAP Skills My Learning screenshot',\n",
    "        'Learn how to learn (Google)',\n",
    "        'Business Communication (Google)',\n",
    "        'Communicate your ideas through storytelling and design (Google)',\n",
    "        'Get started with Microsoft Teams',\n",
    "        'Tech for Good: The Role of ICT in Achieving the SDGs'\n",
    "    ]\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        if idx == 30:\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "\n",
    "        for certificate_column in columns_to_process:\n",
    "\n",
    "            cert_data_list = row[certificate_column]\n",
    "\n",
    "            # Ensure it is a list, if not continue to next iteration\n",
    "            if not isinstance(cert_data_list, list):\n",
    "                continue\n",
    "\n",
    "            for file_name in cert_data_list:\n",
    "                file_path = os.path.join(attachments_dir, file_name)\n",
    "\n",
    "                example_cert_paths = cert_name_to_example_paths.get(certificate_column, [])\n",
    "\n",
    "                for example_cert_path in example_cert_paths:\n",
    "\n",
    "                    cache_key = f\"{file_path}-{example_cert_path}\"\n",
    "\n",
    "                    visual_similarity, text_similarity, learner_id_similarity, cache_df = fetch_cached_similarity(\n",
    "                        cache_df, cache_key, example_cert_path, file_path, learner_id)\n",
    "\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'Certificate Type': [certificate_column],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': ['Success'],\n",
    "                        'Highest Visual Similarity': [visual_similarity],\n",
    "                        'Text Similarity Score': [text_similarity],\n",
    "                        'Learner ID Similarity Score': [learner_id_similarity]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function or Script OLD\n",
    "\n",
    "# Initialize an empty DataFrame for logging results and errors\n",
    "log_df = pd.DataFrame(columns=['Learner ID', 'File Name', 'Status', 'Highest Visual Similarity'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define a batch size\n",
    "    batch_size = 50  # Change this value based on your requirements\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(df_learners_selected) // batch_size + (len(df_learners_selected) % batch_size != 0)\n",
    "\n",
    "    # Initialize log DataFrame\n",
    "    log_df = pd.DataFrame(columns=['Learner ID', 'File Name', 'Status', 'Highest Visual Similarity','Template Match'])\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = (batch_num + 1) * batch_size\n",
    "        batch_df = df_learners_selected.iloc[start_idx:end_idx]\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "        learner_id = row['Learner ID']\n",
    "        file_names = row['File Names_Business Communication']\n",
    "        urls = row['URLs_Business Communication']\n",
    "        downloaded_files = []\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        download_folder = 'temp_downloads'\n",
    "        if not os.path.exists(download_folder):\n",
    "            os.makedirs(download_folder)\n",
    "\n",
    "        for file_name, url in zip(file_names, urls):\n",
    "            temp_file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "            if download_files:\n",
    "                # Download the file only if download_files flag is True\n",
    "                download_file(url, temp_file_path)\n",
    "\n",
    "            try:\n",
    "                if os.path.exists(temp_file_path):\n",
    "                    downloaded_files.append(temp_file_path)\n",
    "                   # Evaluate similarity\n",
    "                    visual_similarity, template_matching_score = evaluate_file_similarity('example_cert.pdf', temp_file_path, template_cert_image) ## define example cert variable\n",
    "\n",
    "                    # You can now log or further process the results\n",
    "                    highest_similarity = max(similarity_scores.values())\n",
    "\n",
    "                    # Log success\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': ['Success'],\n",
    "                        'Highest Visual Similarity': [highest_similarity],\n",
    "                        'Template Match': [template_matching_score]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the exception\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Learner ID': [learner_id],\n",
    "                    'File Name': [file_name],\n",
    "                    'Status': [f'Failed - {str(e)}'],\n",
    "                    'Highest Visual Similarity': [None]\n",
    "                })\n",
    "                log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save log_df to a CSV\n",
    "    log_df.to_csv(\"log.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2086cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD Main Function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize an empty list to hold errors for logging later\n",
    "    error_logs = []\n",
    "\n",
    "    columns_to_process = [\n",
    "        'Boost Your Career with SAP Skills My Learning screenshot',\n",
    "        'Learn how to learn (Google)',\n",
    "        'Business Communication (Google)',\n",
    "        'Communicate your ideas through storytelling and design (Google)',\n",
    "        'Get started with Microsoft Teams',\n",
    "        'Tech for Good: The Role of ICT in Achieving the SDGs'\n",
    "    ]\n",
    "\n",
    "for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "\n",
    "        for certificate_column in columns_to_process:\n",
    "\n",
    "            cert_data_list = row[certificate_column]\n",
    "\n",
    "            # Ensure it is a list, if not continue to next iteration\n",
    "            if not isinstance(cert_data_list, list):\n",
    "                continue\n",
    "\n",
    "            for file_name in cert_data_list:\n",
    "                # Updated line here to include course name\n",
    "                file_path = os.path.join(attachments_dir, certificate_column, file_name)\n",
    "\n",
    "                example_cert_paths = cert_name_to_example_paths.get(certificate_column, [])\n",
    "\n",
    "                for example_cert_path in example_cert_paths:\n",
    "\n",
    "                    try:\n",
    "                        # Your existing code that may throw a FileNotFoundError\n",
    "                        cache_key = f\"{file_path}-{example_cert_path}\"\n",
    "                        visual_similarity, text_similarity, learner_id_similarity, cache_df = fetch_cached_similarity(\n",
    "                            cache_df, cache_key, example_cert_path, file_path, learner_id)\n",
    "\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'Learner ID': [learner_id],\n",
    "                            'Certificate Type': [certificate_column],\n",
    "                            'File Name': [file_name],\n",
    "                            'Status': ['Success'],\n",
    "                            'Highest Visual Similarity': [visual_similarity],\n",
    "                            'Text Similarity Score': [text_similarity],\n",
    "                            'Learner ID Similarity Score': [learner_id_similarity]\n",
    "                        })\n",
    "                        log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "                    except FileNotFoundError:\n",
    "                        error_msg = f\"File not found: {file_path} or {example_cert_path}\"\n",
    "                        print(error_msg)\n",
    "                        error_logs.append(error_msg)\n",
    "                        continue\n",
    "                    except Exception as e:  # New block to catch other exceptions\n",
    "                        error_msg = f\"An exception occurred: {e}\"\n",
    "                        print(error_msg)\n",
    "                        error_logs.append(error_msg)\n",
    "                        continue\n",
    "\n",
    "                # Print or save error_logs as needed\n",
    "                if error_logs:\n",
    "                    print(\"The following errors were encountered:\")\n",
    "                    for error in error_logs:\n",
    "                        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Main Fuinction with caching\n",
    "\n",
    "# Initialize an empty DataFrame for logging results and errors\n",
    "log_df = pd.DataFrame(columns=['Learner ID',\n",
    "                               'Certificate Type',\n",
    "                               'File Name',\n",
    "                               'Status',\n",
    "                               'Highest Visual Similarity',\n",
    "                               'Text Similarity Score',\n",
    "                               'Learner ID Similarity Score'\n",
    "                               ])\n",
    "\n",
    "# Initialize a DataFrame for caching\n",
    "cache_df = pd.DataFrame(columns=['Cache Key', 'Visual Similarity', 'Text Similarity', 'Learner ID Similarity'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        if idx == 30:  # Stop after processing 30 rows\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "\n",
    "        for certificate_column in certificate_names:\n",
    "\n",
    "            cert_data = row[certificate_column]\n",
    "            if pd.isna(cert_data):  # Skip if this cell is NaN\n",
    "                continue\n",
    "\n",
    "            file_name = extract_file_name(cert_data)  # Assuming this function is defined elsewhere\n",
    "            file_path = os.path.join(attachments_dir, file_name)\n",
    "\n",
    "            example_cert_paths = cert_name_to_example_paths.get(certificate_column, [])\n",
    "            if not example_cert_paths:\n",
    "                continue\n",
    "\n",
    "            for example_cert_path in example_cert_paths:\n",
    "\n",
    "                cache_key = f\"{file_path}-{example_cert_path}\"\n",
    "\n",
    "                cached_row = cache_df[cache_df['Cache Key'] == cache_key]\n",
    "\n",
    "                if cached_row.empty:\n",
    "                    visual_similarity = evaluate_file_similarity(example_cert_path, file_path)\n",
    "                    text_similarity = calculate_text_similarity(example_cert_path, file_path)\n",
    "                    learner_id_similarity = calculate_learner_id_similarity(learner_id, file_path)\n",
    "\n",
    "                    new_cache_row = pd.DataFrame({\n",
    "                        'Cache Key': [cache_key],\n",
    "                        'Visual Similarity': [visual_similarity],\n",
    "                        'Text Similarity': [text_similarity],\n",
    "                        'Learner ID Similarity': [learner_id_similarity]\n",
    "                    })\n",
    "                    cache_df = pd.concat([cache_df, new_cache_row], ignore_index=True)\n",
    "\n",
    "                else:\n",
    "                    visual_similarity = cached_row['Visual Similarity'].iloc[0]\n",
    "                    text_similarity = cached_row['Text Similarity'].iloc[0]\n",
    "                    learner_id_similarity = cached_row['Learner ID Similarity'].iloc[0]\n",
    "\n",
    "                try:\n",
    "                    visual_similarity = evaluate_file_similarity(example_cert_path, file_path)\n",
    "                    text_similarity = calculate_text_similarity(example_cert_path, file_path)\n",
    "                    learner_id_similarity = calculate_learner_id_similarity(learner_id, file_path)\n",
    "\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'Certificate Type': [certificate_column],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': ['Success'],\n",
    "                        'Highest Visual Similarity': [visual_similarity],\n",
    "                        'Text Similarity Score': [text_similarity],\n",
    "                        'Learner ID Similarity Score': [learner_id_similarity]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "                except Exception as e:\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'Certificate Type': [certificate_column],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': [f'Failed - {str(e)}'],\n",
    "                        'Highest Visual Similarity': [None],\n",
    "                        'Text Similarity Score': [0],\n",
    "                        'Learner ID Similarity Score': [0]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "    log_df.to_csv(\"log.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e96952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Main Function\n",
    "\n",
    "# Initialize an empty DataFrame for logging results and errors\n",
    "log_df = pd.DataFrame(columns=['Learner ID',\n",
    "                               'Certificate Type',\n",
    "                               'File Name',\n",
    "                               'Status',\n",
    "                               'Highest Visual Similarity',\n",
    "                               'Text Similarity Score',\n",
    "                               'Learner ID Similarity Score'\n",
    "                               ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        # Break the loop after processing 30 rows\n",
    "        if idx == 30:\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "        certificate_columns = certificate_names\n",
    "\n",
    "        for certificate_column in certificate_columns:\n",
    "            cert_data = row[certificate_column]\n",
    "            if pd.isna(cert_data):  # Skip if this cell is NaN\n",
    "                continue\n",
    "\n",
    "            # Split the cell data into file names and URLs (Assuming function returns these)\n",
    "            file_name, url = split_file_url_complex(cert_data)\n",
    "\n",
    "            # Additional code logic for downloading, folder creation etc.\n",
    "            # ...\n",
    "\n",
    "            try:\n",
    "                # Initialize max_similarity values\n",
    "                max_visual_similarity = 0\n",
    "                max_text_similarity = 0\n",
    "                max_learner_id_similarity = 0\n",
    "\n",
    "                # Create the directory if it doesn't exist\n",
    "                download_folder = 'temp_downloads'\n",
    "                if not os.path.exists(download_folder):\n",
    "                    os.makedirs(download_folder)\n",
    "\n",
    "                # Define the path where the file will be downloaded\n",
    "                temp_file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "                # Download the file\n",
    "                download_file(url, temp_file_path)  # Assuming you have a function 'download_file' that takes a URL and a file path\n",
    "\n",
    "                # Check if the example certificate path exists\n",
    "                example_cert_path = cert_name_to_example_path.get(certificate_column, None)\n",
    "                if example_cert_path is None:\n",
    "                    continue  # Skip this iteration if we don't have an example\n",
    "\n",
    "                # Log success with max_similarity values\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Learner ID': [learner_id],\n",
    "                    'Certificate Type': [certificate_column],\n",
    "                    'File Name': [file_name],\n",
    "                    'Status': ['Success'],\n",
    "                    'Highest Visual Similarity': [max_visual_similarity],\n",
    "                    'Text Similarity Score': [max_text_similarity],\n",
    "                    'Learner ID Similarity Score': [max_learner_id_similarity]\n",
    "                })\n",
    "                log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the exception\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Learner ID': [learner_id],\n",
    "                    'Certificate Type': [certificate_column],\n",
    "                    'File Name': [file_name],\n",
    "                    'Status': [f'Failed - {str(e)}'],\n",
    "                    'Highest Visual Similarity': [None],\n",
    "                    'Text Similarity Score': [0],\n",
    "                    'Learner ID Similarity Score': [0]\n",
    "                })\n",
    "                log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "        # Save log_df to a CSV after each learner for safety\n",
    "        log_df.to_csv(\"log.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89992052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older Main Function or Script\n",
    "\n",
    "# Initialize an empty DataFrame for logging results and errors\n",
    "log_df = pd.DataFrame(columns=['Learner ID',\n",
    "                               'Certificate Type',\n",
    "                               'File Name',\n",
    "                               'Status',\n",
    "                               'Highest Visual Similarity',\n",
    "                               'Text Similarity Score',\n",
    "                               'Learner ID Similarity Score'\n",
    "                               ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define a batch size\n",
    "    batch_size = 20  # Changed batch size to 20 for the test run\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(df_learners_selected) // batch_size + (len(df_learners_selected) % batch_size != 0)\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = (batch_num + 1) * batch_size\n",
    "        batch_df = df_learners_selected.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Added a break to run only the first batch as a test\n",
    "        if batch_num == 50:\n",
    "            break\n",
    "\n",
    "        for certificate_name in certificate_names:\n",
    "            col_file_names = f'File Names_{certificate_name}'\n",
    "            col_urls = f'URLs_{certificate_name}'\n",
    "\n",
    "            df_learners_selected[[col_file_names, col_urls]] = df_learners_selected[certificate_name].apply(split_file_url_complex)\n",
    "\n",
    "            for idx, row in batch_df.iterrows():\n",
    "                learner_id = row['Learner ID']\n",
    "                cert_names = row['Certificate Names']  # This should contain the list of certificates for the learner\n",
    "                file_names = row[col_file_names]\n",
    "                urls = row[col_urls]\n",
    "                downloaded_files = []\n",
    "\n",
    "                # Create the directory if it doesn't exist\n",
    "                download_folder = 'temp_downloads'\n",
    "                if not os.path.exists(download_folder):\n",
    "                    os.makedirs(download_folder)\n",
    "\n",
    "                for cert_name, file_name, url in zip(cert_names, file_names, urls):\n",
    "                    temp_file_path = os.path.join(download_folder, file_name)\n",
    "\n",
    "                    if download_files:\n",
    "                        # Download the file only if download_files flag is True\n",
    "                        download_file(url, temp_file_path)\n",
    "\n",
    "                    # Get the corresponding example certificate path using the mapping\n",
    "                    example_cert_path = cert_name_to_example_path.get(cert_name, None)\n",
    "                    if example_cert_path is None:\n",
    "                        continue  # Skip this iteration if we don't have an example cert for this certificate name\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        if os.path.exists(temp_file_path):\n",
    "                            downloaded_files.append(temp_file_path)\n",
    "\n",
    "                            # Loop through all example paths for this certificate type\n",
    "                            for example_cert_path in cert_name_to_example_paths.get(cert_name, []):\n",
    "\n",
    "                              # Evaluate visual similarity\n",
    "                              visual_similarity = evaluate_file_similarity(example_cert_path, temp_file_path) ## define example cert variable\n",
    "\n",
    "                              # Extract text from the downloaded file\n",
    "                              extracted_text = extract_text_from_file(temp_file_path)\n",
    "\n",
    "                              # Compute text similarity to key phrases\n",
    "                              text_similarity_score = calculate_text_similarity_key_phrases(key_phrases, extracted_text)\n",
    "\n",
    "                              # Compute Learner ID similarity\n",
    "                              learner_id_similarity_score = calculate_learner_id_similarity(extracted_text, learner_id)\n",
    "\n",
    "                            # Log success\n",
    "                            new_row = pd.DataFrame({\n",
    "                                'Learner ID': [learner_id],\n",
    "                                'Certificate Type': [certificate_name],\n",
    "                                'File Name': [file_name],\n",
    "                                'Status': ['Success'],\n",
    "                                'Highest Visual Similarity': [visual_similarity],\n",
    "                                'Text Similarity Score': [text_similarity_score],\n",
    "                                'Learner ID Similarity Score': [learner_id_similarity_score]\n",
    "                            })\n",
    "                            log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # Log the exception\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'Learner ID': [learner_id],\n",
    "                            'Certificate Type': [certificate_name],\n",
    "                            'File Name': [file_name],\n",
    "                            'Status': [f'Failed - {str(e)}'],\n",
    "                            'Highest Visual Similarity': [None],\n",
    "                            'Text Similarity Score': [0],\n",
    "                            'Learner ID Similarity Score': [0]\n",
    "                        })\n",
    "                        log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save log_df to a CSV\n",
    "    log_df.to_csv(\"log.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Main Function\n",
    "\n",
    "# Initialize an empty DataFrame for logging results and errors\n",
    "log_df = pd.DataFrame(columns=['Learner ID',\n",
    "                               'Certificate Type',\n",
    "                               'File Name',\n",
    "                               'Status',\n",
    "                               'Highest Visual Similarity',\n",
    "                               'Text Similarity Score',\n",
    "                               'Learner ID Similarity Score'\n",
    "                               ])\n",
    "\n",
    "# Define the directory where attachments are stored\n",
    "attachments_dir = '/content/drive/MyDrive/Colab Notebooks/Certificate confirmer data/attachments'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for idx, row in df_learners_selected.iterrows():\n",
    "\n",
    "        if idx == 30:  # Stop after processing 30 rows\n",
    "            break\n",
    "\n",
    "        learner_id = row['Learner ID']\n",
    "\n",
    "        for certificate_column in certificate_columns:  # Assuming certificate_columns is defined elsewhere\n",
    "\n",
    "            cert_data = row[certificate_column]\n",
    "            if pd.isna(cert_data):  # Skip if this cell is NaN\n",
    "                continue\n",
    "\n",
    "            file_name = extract_file_name(cert_data)  # Assuming this function is defined elsewhere\n",
    "            file_path = os.path.join(attachments_dir, file_name)\n",
    "\n",
    "            example_cert_paths = cert_name_to_example_paths.get(certificate_column, [])\n",
    "            if not example_cert_paths:\n",
    "                continue\n",
    "\n",
    "            for example_cert_path in example_cert_paths:\n",
    "\n",
    "                try:\n",
    "                    visual_similarity = evaluate_file_similarity(example_cert_path, file_path)\n",
    "                    text_similarity = calculate_text_similarity(example_cert_path, file_path)\n",
    "                    learner_id_similarity = calculate_learner_id_similarity(learner_id, file_path)\n",
    "\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'Certificate Type': [certificate_column],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': ['Success'],\n",
    "                        'Highest Visual Similarity': [visual_similarity],\n",
    "                        'Text Similarity Score': [text_similarity],\n",
    "                        'Learner ID Similarity Score': [learner_id_similarity]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "                except Exception as e:\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'Learner ID': [learner_id],\n",
    "                        'Certificate Type': [certificate_column],\n",
    "                        'File Name': [file_name],\n",
    "                        'Status': [f'Failed - {str(e)}'],\n",
    "                        'Highest Visual Similarity': [None],\n",
    "                        'Text Similarity Score': [0],\n",
    "                        'Learner ID Similarity Score': [0]\n",
    "                    })\n",
    "                    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "\n",
    "    log_df.to_csv(\"log.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
